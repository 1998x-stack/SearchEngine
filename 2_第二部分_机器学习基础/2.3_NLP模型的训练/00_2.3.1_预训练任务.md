# 00_2.3.1_预训练任务

"""
Lecture: 2_第二部分_机器学习基础/2.3_NLP模型的训练
Content: 00_2.3.1_预训练任务
"""

### 预训练任务的极致详细分析

#### 一、任务综述
预训练任务是自然语言处理（NLP）模型训练的关键步骤之一，通过利用海量无标签数据进行自监督学习，为下游任务提供高质量的预训练模型。预训练任务的主要目标是让模型学习语言的基本结构和语义信息，从而在后续的特定任务中具有更好的表现。

#### 二、预训练任务的类型

##### 1. Masked Language Model（MLM）
MLM 是 BERT 模型中的核心预训练任务，其目标是预测被遮挡的词。具体方法是随机遮挡输入句子中 15% 的词，并要求模型根据上下文预测这些被遮挡的词。其步骤如下：
- 随机选择句子中的若干个词（通常是 15%）。
- 将这些词替换为特殊标记 [MASK]。
- 模型根据上下文预测被遮挡的词 。

例如，句子“机器学习是人工智能的一个分支”可能被处理为“机器[MASK]是人工[MASK]的一个分支”，模型需要预测被遮挡的词“学习”和“智能”。

##### 2. Next Sentence Prediction（NSP）
NSP 是 BERT 模型中的另一项预训练任务，目标是判断两句话是否是连续的。其步骤如下：
- 将两句话拼接起来，分别标记为正样本和负样本。正样本是原文中连续的两句话，负样本是随机选择的两句话。
- 模型需要判断两句话是否连续  。

例如，句子对“[CLS]机器学习是人工智能的一个分支[SEP]它由数据驱动[SEP]”是正样本，而“[CLS]机器学习是人工智能的一个分支[SEP]天气晴朗[SEP]”是负样本。

##### 3. Sentence Order Prediction（SOP）
SOP 是 ALBERT 模型提出的预训练任务，类似于 NSP，但目标是判断两句话的顺序是否被颠倒。其步骤如下：
- 将两句话拼接起来，一种情况是按原顺序，另一种情况是将顺序颠倒。
- 模型需要判断两句话的顺序是否正确 。

例如，句子对“[CLS]机器学习是人工智能的一个分支[SEP]它由数据驱动[SEP]”是正样本，而“[CLS]它由数据驱动[SEP]机器学习是人工智能的一个分支[SEP]”是负样本。

#### 三、预训练任务的好处
预训练的主要好处在于它可以利用海量的无标签数据进行训练，从而学习到丰富的语言表示，这对下游的各种任务（如文本分类、问答系统等）都有显著的提升。以下是预训练的具体好处：

##### 1. 数据规模
数据规模是预训练的关键因素。大规模数据可以显著提高模型的性能。例如，在 100 亿条样本上训练 1 epoch 的效果优于在 10 亿条样本上训练 10 epoch，即使两者所需的算力相同  。

##### 2. 数据质量
数据质量直接影响预训练的效果。预训练前需要对文档进行去重，并清洗数据中的无意义符号和表情  。

##### 3. 数据选择
根据下游任务选择预训练数据可以显著提升效果。例如，如果下游任务是小红书的搜索相关性，那么在预训练中使用小红书的数据会有很大提升 。

#### 四、实际应用中的注意事项

##### 1. 数据清洗
在进行预训练前，需要对数据进行彻底的清洗，包括去除无意义的符号和表情，并对文档进行去重  。

##### 2. 特征选择
根据下游任务的需求，选择合适的特征进行预训练。例如，在搜索相关性任务中，可以结合词频、逆文档频率（TF-IDF）、词向量等特征  。

##### 3. 模型调优
通过选择合适的超参数、使用正则化技术和集成方法，可以进一步提升模型的稳定性和准确性  。

#### 五、总结
预训练任务在自然语言处理模型的训练中起到了至关重要的作用。通过合理选择预训练任务和数据，可以显著提升模型在下游任务中的表现。无论是 Masked Language Model、Next Sentence Prediction 还是 Sentence Order Prediction，都为模型提供了丰富的语言知识和语义表示，从而在后续的任务中表现出色   。
---
### 预训练任务类型的极致详细比较表

| 比较维度             | Masked Language Model（MLM）                           | Next Sentence Prediction（NSP）                      | Sentence Order Prediction（SOP）                        |
|---------------------|------------------------------------------------------|--------------------------------------------------|------------------------------------------------------|
| **定义**             | 预测被随机遮挡的词                                      | 判断两句话是否连续                                      | 判断两句话的顺序是否被颠倒                                    |
| **目标**             | 让模型根据上下文预测被遮挡的词                               | 让模型判断两句话是否连续                                   | 让模型判断两句话的顺序是否正确                                  |
| **常用模型**           | BERT、RoBERTa                                        | BERT                                             | ALBERT                                               |
| **具体步骤**           | - 随机选择句子中的若干个词（通常是 15%）<br> - 将这些词替换为特殊标记 [MASK]<br> - 模型根据上下文预测被遮挡的词 | - 将两句话拼接起来<br> - 标记为正样本或负样本<br> - 模型判断两句话是否连续       | - 将两句话拼接起来<br> - 一种情况是按原顺序<br> - 一种情况是将顺序颠倒<br> - 模型判断顺序是否正确 |
| **输入示例**           | 原句子：“机器学习是人工智能的一个分支”<br> 处理后：“机器[MASK]是人工[MASK]的一个分支” | 正样本：“[CLS]机器学习是人工智能的一个分支[SEP]它由数据驱动[SEP]”<br> 负样本：“[CLS]机器学习是人工智能的一个分支[SEP]天气晴朗[SEP]” | 正样本：“[CLS]机器学习是人工智能的一个分支[SEP]它由数据驱动[SEP]”<br> 负样本：“[CLS]它由数据驱动[SEP]机器学习是人工智能的一个分支[SEP]” |
| **优点**             | - 能有效学习词汇和上下文之间的关系<br> - 提供丰富的词向量表示               | - 提供句子级别的语义理解<br> - 对上下文有更好的理解                     | - 提供句子顺序级别的语义理解<br> - 对上下文有更好的理解                   |
| **缺点**             | - 仅关注词汇级别的预测<br> - 无法提供句子级别的语义关系                 | - 需要额外的句子对数据<br> - 训练复杂度较高                         | - 需要额外的句子对数据<br> - 训练复杂度较高                         |
| **应用场景**           | - 语言模型的预训练<br> - 词向量表示的学习                         | - 文本分类<br> - 文本匹配<br> - 自然语言推理                   | - 文本分类<br> - 文本匹配<br> - 自然语言推理                     |
| **对下游任务的影响**      | - 提升任务中对单词和上下文理解的效果                             | - 提升任务中对句子连续性和上下文理解的效果                         | - 提升任务中对句子顺序和上下文理解的效果                         |
| **实现复杂度**         | 低                                                 | 中                                                 | 中                                                 |
| **数据需求**           | 大量无标签文本数据                                         | 大量句子对数据                                            | 大量句子对数据                                            |
| **计算资源需求**        | 中等                                                | 较高                                                | 较高                                                |
| **代表性论文**         | BERT（2018）                                         | BERT（2018）                                         | ALBERT（2019）                                       |
